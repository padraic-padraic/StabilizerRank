\documentclass{standalone}
\begin{document}
%Structure for the Lit Review
%Discuss origins of quantum computing, Quantum Turing Mahcine, Quantum Speedup
%Discuss constraints e.g. Bennet & Brassard on no NP speedup
%Discuss belief of the origins of this speedup e.g Entanglement (Jozsa, Vidal, Van Den Nest), Contextuality
% New Section: Fault Tolerant Quantum Devices
% GK: Stab circuits are efficiently simulable & stricly weaker than classical comp
% Need a Cn>3 gate => State injeciton
% Discuss magic states, costly to implement
% Magic states coincide with the contextuality definition!
% Discuss simulation of CLifford +T, and seeming exponential value of the T under CHP
The origin of quantum computation is typically attributed to Professor Feynman, in a keynote address at the 1981 `First Conference on the Physics of Computation' at MIT~\cite{Feynman1982}. Feynman's speech discussed the problems of simulating physical systems computationally. As one candidate solution, he proposed the technique of quantum simulation: using the controlled dynamics of one quantum system to probe the dynamics of another. 
\par
At the same conference, Peter Benioff presented a paper on using Hamiltonian evolution of a spin-lattice system to realise a Turing Machine~\cite{Benioff1986}, an abstract model used to study universal classical computation. The state of the system is represented by an `tape', broken into cells which store binary values. The computation is carried out by a `head', which can read and write bits and move along the tape accoding to a programme. This simplified model is often employed in complexity theory, where the number of `steps' taken by the head and number of cells used give the temporal and spatial resources consumed in the computation.  In essence, Benioff proposed a quantum implementation of classical computation, encoding the binary state in the spin of the lattice, where the reading and writing would be controlled by spin-spin interactions. 
\par
Feynman's proposed quantum simulator followed a similar scheme, premised on applying annihilation, creation and `number' operators to different components of the system. This is analagous to subtracting, adding or reading a value on a Turing machine tape. Feynman similarly noted that a two-level spin system would serve as the smallest implementation of such a device. 
\par
Both schemes are early proposal of the `qubit', a two-level quantum system used as a building block in computation and communication protocols. The earliest example is attributed to Stephen Wiesner, who discussed using polarisation states of photons to implement a secure serial number in a `Quantum Money' scheme~\cite{Wiesner1983}. Wiesner developed this scheme in the 70s, but was unable to publish the work until after Feynman and Benioff's talks~\cite{Brassard2006}. The term qubit was later coined by Schumacher, in his paper on a quantum analogue of Shannon's noiseless coding theorem~\cite{Schumacher1995a}.
\par
The `quantum advantage' in these early proposals is presrnted somewhat abstractly. In Benioff's paper, he argues that implementing computation directly on a physical system should offer speed, size and energy advantages over existing classical implementations, but doesn't discuss these effects as originating from the quantum mechanical nature of the system. In Feynman's talk, however, the advtanges of using a quantum simulator are summarised succinctly in the popular quote:
\textquote{Nature isn't classical, dammit, and if you want to make a simulation of nature you better make it quantum mechanical!}
\par

\section{Quantum Turing Machines}\label{sec:QTM}
This question of what computational resources we need to simulate arbitrary physical systems was formalised by Deutsch in his 1985 paper `Quantum theory, the Church-Turing principle, and the universal quantum computer'. This paper is widely considered as establishing the theory of quantum computation, as it formally defines the principle of `Universal Quantum Computing.' 
\par
In this paper, Deutsch relates the Church-Turing thesis, a statement about the nature of computationally solveable problems, to the problem of simulating physical systems Feynman highlighted, to give a novel definiton of a universal computing machine: any device capable of perfectly simulating a finitely realisable physical system with finite resources. The classical Turing Machine fails this definition as binary numeral systems cannot be used to represent continuous real numbers require to simulate classical physical systems. Jozsa describes this strengthened Church-Turing-Deutsch thesis as requiring that questions of `What is computable?'\ be grounded in the laws of physics and not characterised by mathematics alone~\cite{Jozsa1997}.
\par
Instead, Deutsch introduces a model of a Quantum Turing Machine, which passes this Church-Turing-Deutsch criterion for simulating finite quantum mechanical systems. This is an extension of Feynman's ideas, and the quantum measurement automaton proposed by Albert~\cite{Albert1983}. Unlike Benioff's proposal, which restricted the system to classical computational states, the state of this device is explicity quantum mechanical, allowing any state in the Hilbert space $\mathcal{H}=\mathbb{C}^{2^{n}} \otimes \mathbb{C}^{2^{m}}$ defined by the $n$ qubits in the processor $m$ qubits in the `tape'. The evolution of the combined processor-tape system is the described with unitary dynamics. 
\par
Interestingly, Deutsch proves that a Quantum Turing Machine would still be restriced to solving the class of `General Recursive' functions defined as computable under the Church-Turing thesis~\cite{Deutsch1985}. However, he goes on to demonstrate that a Quantum Turing Machine is capable of simulating a classical computation, and of simulating a much broader class of physical systems than a classical device~\cite{Deutsch1985}.
\par
In this paper, Deutsch also set out an algorithm which demonstrates the ability of a quantum computer to perform tasks faster than any classical device. In particular, he constructs a toy problem where we are given access to an `orcale', which evaluates a function $f:\mathbb{Z}_{2}^{2}\rightarrow\{0,1\}$ and returns the result. We are tasked with determining if the function is equal for all inputs (`constant'), or if it returns $0$ for half of the inputs and $1$ for the other (`balanced'), in as few evaluations as possible. An extension of this problem to n-bit functions $f:\mathbb{Z}_{2}^{n}\rightarrow\mathbb{Z}_{2}$, called the Deutsch-Jozsa algorithm, was proposed in 1992, and more dramatically shows the computational advantage of the quantum Turing machine~\cite{Deutsch92}.
\par
To answer this quesiton deterministically, a classical computer would need to evaluate $f$ for $\frac{2^{n}}{2}+1$ inputs. However, it can be shown that by preparing a quantum computer in a superposition of all $n$-bit binary strings $\ket{\psi}=\frac{1}{\sqrt{2}^{n}}\sum_{x\in\mathbb{Z}_{2}^{n}}\ket{x}$, it sufficies to evaluate $f$ once. Deutsch dubbed this behaviour `quantum parallelism', as it relies both on the ability of quantum states to exist in superposition, and for the existince of relative phase between states such that interference results in a deterministic answer.
\par

\section{The Nature of Quantum Speedup}\label{sec:WhatSpeedup}
The Deutsch-Jozsa algorithm demonstrates a dramatic reduction in `query complexity', but this is relative to a classical algorithm sequentially testing inputs to be able to give a deterministic answer. In reality, a classical computer testing random inputs would be able to answer the Deutsch-Jozsa problem with a certain probability in much less than $\frac{2^{n}}{2}$ trials. 
\par
In classical computational complexity theory, problems of this type belong to a class called `Bounded-error Probability Polynomial'($\BPP$); the class of problems where an efficient algorithm including a randomised process can return the correct answer with an error bounded by a constant independent of the problem size~\cite{Bennett1997}. The quantum analogue of this class, Bounded-error Quantum Polynomial($\BQP$), is considered the class of problems efficiently solvable on a quantum computer.
\par
The term `polynomial' in the above definitions refers to the `time' taken to complete the computation, as a function of the number of bits in the problem $n$. In the Turing machine picture, this is the number of steps in the computation, but it can equivalently be defined as the number of logic gates acting on $n$ bits in a boolean circuit. In 1993, Yao demonstrated that an equivalent `circuit' model exists for quantum computing, and that any quantum circuit is capable of simulating a quantum Turing machine~\cite{Yao1993}. This circuit model, where elementary unitary matrices are acted in succession on $n$-qubits to implement a computation $U$, is the framework used to develop and analyse quantum algorithms. 
\par
Following the work of Deutsch and Jozsa, the question of quantum speedup became: do there exist problems which are efficient\footnote{`Efficient' problems are generally defined as those in the classes $\P$ or $\BPP$ for classical computers, and $\BQP$ for a quantum computer.} for a quantum computer to solve, but which are `harder' than $\BPP$ for a classical device?
\par

\subsection{Beating BPP}\label{sec:exponentialspeedup}
This quesiton was answered in a series of three papers, which both established the existence of such problems and then constructed explicit quantum algorithms in $\BQP$ which, for a classical computer, require an exponential number of gate operations as a fucntion of the problem size~\cite{Bennett1997}.
\par
The existence of this exponential quantum speedup over classical computation was first proved by Bernstein and Vazirani in their paper on quantum computational complexity theory. As well as demonstrating the possibility of exponential speedup, this paper formalised Deutsch's quantum Turing machine construction, and used it to prove a number of properties about $\BQP$; in particular, they demonstrated that $\BPP\subseteq\BQP$.
\par
The first explicit construction of such a problem was given by Simon in 1994, again using an `oracle' formulation of the computation~\cite{Division1998a}. In Simon's problem, the orcale implements a binary function $f: \mathbb{Z}_{2}^{n}\rightarrow\mathbb{Z}_{2}^{n}$, such that for two binary strings $x,y\in\mathbb{Z}_{2}^{n},\;f(x)=f(y)\Leftrightarrow x = y \oplus s$, for some constant string $s$. For a quantum computer, this algorith requires $O(n)$ oracle queries, unlike a classical method which requires $O(2^{\frac{n}{2}}$~\cite{Division1998a}.
\par
Simon's algorithm is another example of a `toy' problem, but the techniques presented were then applied by Peter Shor to develop a general quantum algorithm capable of solving two mathematical problems: Prime Factorisation and the Discrete Logarithm~\cite{Division1998a,Shor1997}. In complexity theoretic terms, they belong to the class $\NP$, of problems which require exponential time to solve, but for which a proposed solution can be checked efficiently~\cite{Bennett1997}.
\par
It is believed that a polynomial-time classical algorithm for both problems does not exist. In face, the presumed computational hardness of prime factorisation is what guarantees the security of RSA and other cryptographic protocols currently relied upon for secure encryption. The discovery of an efficient quantum algorithm spurred a great deal of interest in quantum computing, as it raised the possibility of devices capable of breaking most if not all modern cryptography~\cite{Shor1997}. 
\par
The existence of problems that reside in the class $\BQP$ but for which no efficient classical algorithm exists, demonstrates further the ideas of Feynman and Deutsch that a classical computer cannot efficiently simulate a quantum system. Any efficient simulation of arbitrary quantum systems would be also be able to implement quantum computations, and would in turn give us an eficient classical algorithm for prime factorisation~\cite{Shor1997}.
\par

\subsection{NP-Complete Problems}\label{sec:notNP}
While prime factorisation and the discrete logarithm are difficult to solve classicaly, they do not belong to the so called $\NP$-complete problems; problems which belong to the class $\NP$, and for which any \emph{other} problem in $\NP$ can be encoded as an instance of the problem~\cite{ComplexityZoo}. As a result of this `completeness' property, the $\NP$-complete problems are considered some of the most import in computer science~\cite{Bennett1997}.
\par
In 1996, Lov Grover developed a quantum algorithm capable of solving arbitrary $\NP$ problems, including the $\NP$-complete problems, using the polynomial `verifier' of a candidate solution implemented as an oracle~\cite{Grover1996}. This algorithm is commonly reffered to as `quantum search', as it was developed as a technique for searching for a single item in a set of $N$ entries. A classical, brute-force search would, in the worst case, require testing all $N$ items, an in general $O(N)$. 
\par
Grover's quantum algorithm, however, can succeed in $O(\sqrt{N})$ trials, a more modest quadratic reduction in the oracle complexity. The techniques employed are very similar to the Deutsch-Jozsa algorithm, applying the oracle to a uniform superposition of $N$ inputs. This is followed by the application of a `diffusion' operator, which boosts the amplitude of the state corresponding to the solution. This amplitude is maximal after $O(\sqrt{N})$ applications of this pair of gates~\cite{Grover1996}.
\par
A brute-force search is likely not optimal for realistic database searching. For example, if the data can be sorted based on the desired criteria, then a `bracketing and bisection' technique based on testing random entries can succeed in $O(\log_{2}(N))$ trials. However, this kind of brute force problem does arise when considered $\NP$-complete problems, such as optimisation problems. In this case, Grover's algorithm allows a quantum computer to achieve a quadratic speedup for any $\NP$-complete problem. Shortly afterwards, it was shown by Bennett et al.\ that Grover's quadratic speedup is likely optimal for any quantum algorithm tackling $\NP$-complete problems~\cite{Bennett1997}.
\par
These results have demonstrated the nature of Quantum Speedup. Quantum computing seems proveably stronger than classical computation when it comes to simulation of physical systems, as it satisifes the Church-Turing-Duetsch thesis. It has also been proven that there exist problems believed to be in $\NP$, which reside in $\BQP$ for a quantum computer. This in turns suggests that$\P\cup\BPP\subseteq\BQP$. However, it is believed that quantum computers cannot offer a greater than quadratic speedup for the $\NP$-complete problems. 

\section{What is the Origin of Quantum Speedup?}
Given the existence of quantum speedup, it is interesting to ask what aspects of quantum mechanics underlie this effect. This is not only an interesting theoretical quesiton, but also relveant to our attempts to realise quantum technologies. Control of quantum systems, including isolating them from the effects of decoherence, represents a significant challenge~\cite{Shor1997}. Understanding the origin of quantum speedup also helps us to understand what criteria our experimental devices will need to satisfty.
\par
In section~\ref{sec:QTM}, we introduced the notion of `quantum parallelism', the ability of a quantum system to exist in superpositions of multiple states, for an oracle funciton to be evaluated simulataneously across all inputs. Holevo's bound tells us, however, that while it is possible for the system to be in a superposition of $2^{n}$ states, we can only access classical information of $1$ of these states~\cite{holevo1973}. As such, we also require interference effects to be present in the computation.
\par
However, in a 1997 paper, Jozsa discusses the fact that superposition and interference are both effects demonstrated by classical waves~\cite{Jozsa1997}. Instead, Jozsa argues that the critical feature of quantum mechanics is the existence of entangled states. 
\par

\subsection{Entanglement and Qauntum Computing}\label{sec:entanglement}
A general quantum state on $n$-qubits, $\ket{\psi}$, exists in the Hilbert space $\mathcal{H}_{2^{n}}$. If this state can be decomposed into a tensor product of states on smaller Hilbert spaces, then it is called a `separable' or `product' state. If no such decomposition exists, the state is said to be entangled. This definition is exact for pure states, though entanglement for mixed states is much less well characterised~\cite{Laflamme2001}. 
\par
In his paper, Jozsa considers the example of a classical system in a superposition of $2^{n}$ states, constructed out of $n$ strings each in a superposition of their two lowest-energy vibrational modes. However, classical systems combine under the cartesian product, in contrast to quantum systems. As a result, this physical system remains a product of $n$ indivudual subsytems. The phenomenon of entanglement, where a physical system cannot be described as a combination of smaller subsystems, is not possible clasically~\cite{Jozsa1997}. 
\par
This argument was extended in a later paper in collaboration with Ekert~\cite{Ekert1998}. In this paper, they also consier the possibility of constructuing a classical simulator based on a superposition of $2^{n}$ vibrational modes, which would epxloit an isomorphism between $\otimes^{n}\mathcal{H}_{2}$, the Hilbert space n qubits, and $\mathcal{H}_{2^{n}}$, the Hilbert space of one, $2^{n}$-level quantum system. In this picture, the `isomorphism' would allow us to label certain states of the classical simulator as entangled~\cite{Ekert1998}. 
\par
However, the authors demonstrate that such a classical simulator cannot be implemented without an exponential overhead in some physical resource. For example, a system of $n$-qubits, each with the same `energy gap', would require an amount of energy to control that grows linearly in $n$. However, for the $2^{n}$ level classical simulator, the number of energy levels and thus the energy required to implement it grows exponentially in $n$~\cite{Ekert1998}.
\par
A similar argument applies if we instead consider a different classical simulator where the $2^{n}$ levels accumulate below a finite upper bound~\cite{Jozsa2003}. In this case, the precision required to resolve these levels grows eponentially in $n$ and the simulator is not physically feasible. Thus, the authors argue, the phenomenon of quantum speedup is due precisely due to the existence of entangled states.  
\par
Jozsa and Ekert also indicate the presence of entanglement in existing descriptions of quantum algorithms. For example, when an oracle gate, $O_{f}$, is applied to a superposition of input strings $\frac{1}{2^{n}}\sum_{x\in\mathbb{Z}_{2}^{n}}\ket{x}$, the resulting state is an entangled state of each input with its evaluation $\sum_{x\in\mathbb{Z}_{2}^{n}}\ket{x}\ket{f(x)}$~\cite{Jozsa1997}. Ekert and Jozsa also demonstrate the existence of entanglement in the Quantum Fourier Transform, a subrutine employed in Simon's and Shor's algorithms~\cite{Ekert1998}.

\subsubsection*{Impact on Experimental Efforts}
This identification of entanglement as a the key resource for quantum speedup had a significant impact on the quantum computing community. In particular, it called into question a series of early quantum computing experiments realised using NMR systems~\cite{Braunstein1999,Linden2001} It was shown by Braunstein et al.\ that, for sufficiently high levels of noise, the `pseudo-pure' states used in NMR computing would remain seperable throughout the computation~\cite{Braunstein1999}. This analysis in fact demonstrated that all previous NMR experiments had systematic noise above this threshold. A subsequent paper by Linden and Popsecu explicitly demonstrated that executions of Shor's algorithm requires the system to become entangled, again ruling out previous NMR results~\cite{Linden2001}. A similar result was shown for Grover's algorithm~\cite{Braunstein2001}.

\subsection{Is Entanglement Necessary?}\label{sec:classicalsim}
When trying to understand quantum speedup, it can also be instructive to consider the classical information required to fully describe a quantum system. Any quantum dynamics which can be efficiently simulated classically cannot, by definition, realise quantum speedup. The counterstatement, that a lack of efficient classical simulation implies quantum speedup, is not necessarily true, as it assumes no better classical simulation method exists. Nonetheless, an exponential classical complexity is considered indicative of quantum advtanage~\cite{Jozsa2003}.
\par
If we consider the classical information required to fully characterise a quantum state, then a product state of $n$ qubits $\ket{\psi}=\bigotimes_{i=1}^{j}\ket{\phi}_{j}$, can be completely described with $O(j)$ complex numbers~\cite{Jozsa1997}. In contrast, entangled states requires $O(2^{n})$ numbers to describe the system completely~\cite{Jozsa1997}. 
\par
This exponential classical description as a result of entanglement can also be applied to descriptions of quantum circuits. A given computation on $n$ qubits involves a $2^{n}\times2^{n}$ unitary matrix $U$. However, by decomposing this unitary into a sequence of elementary gates acting on one or two qubits, we can reduce this complexity to $O(\poly(n))$~\cite{Ekert1998}. The overall complexity of the computation is then polynomial in $n$ if the state can be described as a product state at all steps~\cite{Ekert1998} 
\par
Jozsa \& Linden used this observation to give a slightly more restricted definition of entanglement as a resource for quantum computation~\cite{Jozsa2003}. In particular, it is not sufficient for only a small, bounded subset of the qubits to become entangled. The authors define the notion of a `p-blocked state', which can be split into subsystems of at most $p$ qubits, and show that a quantum computation is efficiently simulable if at all steps there exists such a decomposition~\cite{Jozsa2003}. Instead, the entanglement in the system must be unbounded, growing to include all qubits present in the computation. They then explicitly demonstrate that, for Shor's algorithm, the entanglement grows in precisely this unbounded way~\cite{Jozsa2003}.

\subsubsection*{Efficient Simulation of Entangled States}\label{sec:gk}
Interestingly, they authors also show that we can approximately simulate our quantum computation to within an error$\eta$, provided there exists at all steps a $p$-blocked state $\epsilon$-close to the `true' state of the computation where $\epsilon=c^{T}\eta$, $T$ is the number of steps in the circuit, and $c$ is a constant less than one~\cite{Jozsa2003}. 
\par
This pair of results significantly constrain the quantum advantage offered by entanglement alone. A similar result was found by Vidal, who developed a novel method for simulating 1D-spin chains with `restricted' entanglement. In particular, he showed that if the entanglement measure $E_{\chi}\equiv\log_{2}(\chi)$, where $\chi$ is the Schmidt-Rank of the state, grows only logarithmically in the system size, then it admits an efficient classical decomposition~\cite{Vidal2003}.
\par
\par
The significance of entanglement as a resource was first quesitoned by Laflamme et al., in response to Branstein's argument against NMR quantum computing on the grounds of separability~\cite{Laflamme2001}. Laflamme argued that, whereas the nonlocal behaviour of entangled systems gives a clear advantage in quantum communication protocols, the significance of entanglement to quantum computing is much less clear. In particular, it was argued that the apparent significance of entanglement could be explained by the computational description employed by Jozsa and Ekert, in terms of $2^{n}$ complex amplitudes~\cite{Laflamme2001}.
\par
As a counter example, Laflamme cites the `Heisenberg picture' of quantum computation developed by Gottesman \& Knill. The Heinsberg picture is a construction of quantum mechanics where time-evolution is carried by the operators rather than by the quantum states~\cite{Nielsen2000}, and the Gottesman \& Knill constrsuction similarly allows the quantum circuits acting on a class of quantum states to be described in terms of their effect on groups of Pauli operators~\cite{Gottesman1999a}. 
\par
Originally developed in the context of quantum error correction codes, Gottesman's formalism is predicated around groups of operators $\mathcal{S}\subseteq\mathcal{P}_{n}$, subgroups of the $n$-qubit Pauli group. Gottesman showed that there exists subgroups on $n$ qubits $\mathcal{S}:\vert\mathcal{S}\vert=2^{n}$, which have a unique +1 eigenstate $\ket{\phi}:s\ket{\phi}=\ket{\phi}\forall s\in\mathcal{S}$. These eigenstates are called `Stabiliser' states, as they are stabilised by the group $\mathcal{S}$.
\par
The action of these stabiliser groups can be entirely described by its generating set, a subset of independent elements which can be multiplied together to produce all other elementsin the group. Because the stabiliser operators have a common eigenstate, the group is abelian, meaning it's $2^{n}$ elements can be described using just $n$ generators. The action of a unitary $U$ acting on a stabiliser state can then be described by evolving the stabiliser generators $s_{i}:i\in\{1,\cdots, n\}$ under the relation $s_{i}' = UsU^{\dagger}$. 
\par
This allows the Stabiliser states to be fully characterised using $n$ sets of $n$ Pauli operators. Examples of Stabiliser states include the computational basis states, but it also includes a number of entangled states. For example, the maximally entangled Bell state $\ket{\Psi}_{+}=\frac{1}{\sqrt{2}}(\ket{01}+\ket{10})$, is stabilised by the generators $X\otimes \mathbb{I}$ and $\mathbb{I}\otimes X$. Interestingly, this state demonstrates `unbounded' two-qubit entanglement.
\par
For a general computation $U$, these stabiliser generators are $2^{n}\times2^{n}$ matrices, so this descirption remains clasically complex. However, for a large number of elementary quantum gates, including the `entangling' CNOT gate, their action on the Stabiliser states can be efficiently computed classically~\cite{Gottesman1999a}.
\par
These operations are commonly referred to as the Clifford group, defined as the matrices~\cite{Gottesman1999a}
\begin{equation}\label{eq:Clifford}
U:UPU^{\dagger}\in\mathcal{P}_{n}\;\forall P\in\mathcal{P}_{n}.
\end{equation}
As they only map Pauli operators to other Pauli operators, their action on the Stabiliser states is simply given by the changes in the Pauli operators that generate each group. 
\par
While this is a restricted set of operations and states, the Stabiliser formalism is nonetheless capable of describing entangled states. It can also be used to simulate intrisicly `quantum' protocols, such as quantum teleportation, efficiently~\cite{Aaronson2004a}.
\par
Laflamme thus argued that apparent requirement of entanglement for quantum speedup could be perhaps understood as a consequence of the amplitude formalism that Jozsa, Ekert, Linden and Popescu used to describe quantum states and their simulation~\cite{Laflamme2001}. Instead, given the efficient simulability of Clifford circuits, and that mixed-state entanglement is not well understood, Laflamme proposed that it is the full unitary dynamics that gives a quantum computer its advantage~\cite{Laflamme2001}.
\par
A recent result that would seem to support Laflamme's arguments came from Van den Nest, who investigated the universality of quantum computation based only on states with a small amount of entanglement under the `Entanglement Entropy' measure~\cite{VanDenNest2013}. Interestingly, even with access to only a small amount of entanglement, he demonstrated that these states can realise arbitrary unitary dynamics, and such be used to realise universal quantum computation. 
\par
Jozsa \& Linden acknowledge Laflamme's argument in the conclusion to their paper, agreeing that this appareny requirment for unbounded entanglement derives from their chosen classical description of the quantum states, and that different descriptions of quantum mechanics would have different sets of efficiently-described states. 
\par
This debate illustrates the difficulty of demonstrating the origins of quantum advantage; the absence of an efficient classical description is not a clear indicator of quantum speedup, as this description depends on the formalism chosen. For each description $\mathcal{D}$, there exists a corresponding property $\text{prop}(\mathcal{D})$ that is required for quantum speedup~\cite{Jozsa2003}. In some sense, we can argue that a system demonstrating quantum speedup must satisfy $\text{prop}(\mathcal{D})\;\forall\mathcal{D}$~\cite{Jozsa2003,Laflamme2001}. However, the existence of an efficient classical method is a clear signifier of the absence of quantum advantage, and so classical simulations of quantum computation are still an effective way to study the `power' of a quantum computation.
\par
% \subsection{Contextuality}\label{sec:context}
\section{Fault Tolerant Quantum Computing}\label{sec:FTQC}
Despite continued improvements in qubit control, quantum computation still has to contend with the effects of decoherence and environmental noise~\cite{Nielsen2000}. This has lead to the notion of fault tolerant quantum computation schemes, designed to protect the quantum information throughout the computation~\cite{Nielsen2000}. 
\par
In a fault tolerant quantum computer, the information is encoded in multiple physical qubits using a quantum error correciton code. These codes can also be described using Gottesman's Stabiliser formalism; the $2^{m}$ valid basis states of the codespace are the $+1$ eigenstates of a stabiliser group with $k=n-m$ generators, acting on $n$ qubits. The computation is then performed using `logical' gates that act on the encoded states~\cite{Gottesman1997}.
\par
If, during a logical gate,  an error occurs on one of the physical bits, it can be detected by a $-1$ outcome when measuring the stabiliser generators~\cite{Gottesman1997}. By measuring the generators at each step in the computation, we can identify and correct any errors that occur.
\par
However, this also requires that we construct our `logical' gates in a fault tolerant way. If the logical gates induce multiple errors on each encoded qubit, then we are unable to deterministically detect and correct them~\cite{Gottesman2009}. Instead, we typically require that our logical gates be `transversal': that they require no multi-qubit gates within a single code block. This prevents an error on one physical bit from spreading to other qubtits, reducing the probability of our code failing~\cite{Nielsen2000,Gottesman2009}.
\par
Unfortunately, it is known that no stabiliser error-correction code can have a set of transversal gates that is also universal for quantum computation~\cite{Eastin2009}. In fact, most stabiliser codes admit only CLifford group operations as their transversal gate set. As the states of the code are entirely characterised by a stabiliser code, this means that not only are these codes not-universal, but their action can be efficiently simulated classically. It was in fact shown by Aaronson \& Gottesman that these Clifford circuits are strictly \emph{weaker} than a classical computer~\cite{Aaronson2004a}. We thus need some way to boost our Clifford circuits to universality, and to do so in a fault tolerant way. 
\par
An important concept in the theory of universal computational gate sets is the `Clifford Heirarchy', a recursive family of unitary operations defined by their actions on the Pauli operators~\cite{Gottesman1999b}. We define level 1 of the heirarchy, $\mathcal{C}_{1}$, as the Pauli operators themselves. Level 2 then corresponds to the Clifford group which, as defined in Eq.~\ref{eq:Clifford}, maps Pauli operators to other Pauli operators. This can alternatively be written
\begin{equation}\label{eq:c2}
\mathcal{C}_{2} \equiv \{U\} : UPU^{\dagger}\in\mathcal{C}_{1}\;\forall P\in\mathcal{P}.
\end{equation}
Thus, level 2 of the heirarchy has the property that it maps all Paulis to operators in level 1, the set of Pauli operators. We can thus extend this to define level $n$ as
\begin{equation}\label{eq:heirarchy}
\mathcal{C}_{n}\equiv\{U\} : UPU^{\dagger}\in\mathcal{C}_{n-1}\;\forall P\in\mathcal{P}
\end{equation}
It has been shown that a Clifford circuit can be boosted to universal quantum computation if we add just a single gate from $\mathcal{C}_{n\geq3}$. Most commonly, fault tolerant techniques have focused on implementing gates from $\mathcal{C}_{3}$, such as the $T$ gate 
\[T\equiv\begin{pmatrix}1&0\\0&\mathe^{1\frac{\pi}{4}}\end{pmatrix}\]
and the `Toffoli' or `Controlled-Controlled NOT' gate~\cite{Shor1997,Gottesman1999b}. 

\subsection{`Magic' State Injection}\label{sec:MSI}
\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    \includegraphics[width=0.4\textwidth]{Figures/1-bit-Teleport.png}
\caption{Circuit for `1-bit teleportation', taken from~\cite{Zhou2000}.}
\end{wrapfigure}
A fault tolerant T gate can be realised using a `1-bit teleportation' or state-injeciton circuit, developed by Chuang et al~\cite{Zhou2000}. This simplified teleportation circuit made up of an ancillae qubit $H\ket{0}=\ket{+}$, Clifford gates, and an additional measurement-controlled correction.
\par
By using an appropriately prepared ancilla, this circuit can be used to realise the state $U\ket{\psi}$. To find the required state, we need to commute the operator $U$ through the circuit, which can be done for any diagonal unitary $U\in\mathcal{C}_{n}\forall n\geq 3$~\cite{Zhou2000}. This has the affect of changing the required correction operation $X\rightarrow UXU^{\dagger}$, and changing the ancilla $\ket{+}\rightarrow U\ket{+}$~\cite{Zhou2000}.
\par
For the $T$ gate, which satisfies the diagonal requirement, this requires preaparing an ancilla $\ket{A}=\ket{0}+\mathe^{i\frac{\pi}{4}}$. The problem of a fault tolerant implementation then reduces to preparing these states.
\par
A scheme for preparing these ancillae was proposed by Bravyi \& Kitaev, allowing an arbitrarily pure state to be prepared from multiple, imperfectly prepared states~\cite{Bravyi2005}. This scheme can be used to prepare $\ket{H}=\cos(\frac{\pi}{8})\ket{0}+\sin(\frac{\pi}{8})\ket{1}$, and a whole family of other states that are equivalent to $\ket{A}$ up to a Clifford group operation~\cite{Bravyi2005}.
\par
Because they can boost a stabiliser code to universality, these states are dubbed `magic states'~\cite{Bravyi2005}. Bravyi \& Kitaev derived a threshold fidelity for the initial noisy magic states for the distillation procedure to succeed, which corresponds to an octohedron in the Bloch Sphere defined by the single qubit stabiliser states~\cite{Bravyi2005}. $\ket{A}$ and other $T$-gate magic states are pure states that lie on the edges of the octohedron.
\par
An alternative type of magic state, which we will dub $\ket{F}$ for `face'-state, lies at the centre of the faces of this octohedron. This is represented schematically in Fig.~\ref{fig:octohedron}. However, because there is currently no known deterministic procedure for using face-type states in state injection, fault tolerant computing schemes focus on using circuits built out of Clifford gates and $T$ gates.
\par
\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{Figures/octohedron.pdf}
\caption{Representation of the set of clasically simulable states in the Bloch Sphere, show the edge and face-type magic states.}
\label{fig:octohedron}
\end{figure}
\subsubsection{Simulating Clifford+T Circuits}\label{sec:CHP}
This octohedron corresponds to convex mixtures of stabiliser states, and as such it defines the set of efficiently simuable single-qubit states~\cite{Howard2014}. The single qubit Clifford group operations correspond to symmetric rotations of the octohedron, which also serves to illustrate the clasically simulability of clifford circuits on stabiliser states. 
\par
An efficient algorithm for clasically simulating Clifford circuits, called `CHP'\footnote{Named for the generators of the n-qubit Clifford group: CNOT, Hadamard and Phase}, was developed by Aaronson \& Gottesman. Using the fact that the Pauli group can be generated by $\pm\mathi, X$ and $Z$, and that stabiliser generators must must have real-valued phase as the group cannot contain the element $-\mathbb{I}$, they represent each stabiliser on $n$ qubits as a $2n+1$ bit binary string. Pairs of the first $2n$ bits describe the Pauli operator on each of thr $n$ qubits: \texttt{00} corresponds to $\mathbb{I}$, \texttt{01} to $Z$, \texttt{10} to $X$ and $\texttt{11}$ to $Y$. The final bit gives the phase of $\pm 1$.
\par
These $n$ stabiliser bit strings are stored in a $2n\times(2n+1)$ tableau, which also contains the `destabilisers', a dual set of operators that, together with the stabiliser generators generate the full $n$-qubit Pauli group. The \texttt{CHP} method is then capable simulating arbitrary Clifford group operations and pauli mesurements on $n$ qubits in a time $\order{n^{3}}$.
\par
In the paper, Aaronson \& Gottesman also discuss extensions of the CHP method to mixed states, and to Clifford+T circuits. 
\ifstandalone
\bibliography{../MResProject.bib,../ManualEntries.bib}
\fi
\end{document}
